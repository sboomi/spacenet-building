{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import json\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterio.mask import mask\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import fiona\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.transforms as T\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing steps\n",
    "\n",
    "The preprocessing steps's goal is to find the best stats to equilibrate the dataset and the model. In order to standardize the data, we need the average on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "# Main setting\n",
    "train_path = Path('train')\n",
    "test_path = Path('test')\n",
    "\n",
    "print(train_path)\n",
    "print(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>BuildingId</th>\n",
       "      <th>PolygonWKT_Pix</th>\n",
       "      <th>PolygonWKT_Geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AOI_3_Paris_img485</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((31.68 11.69 0,31.06 11.39 0,44.13 -0...</td>\n",
       "      <td>POLYGON ((2.242656935000071 49.023104327000055...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOI_3_Paris_img485</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((-0.0 199.48 0,-0.0 216.43 0,10.24 20...</td>\n",
       "      <td>POLYGON ((2.242571399947189 49.022597304268373...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AOI_3_Paris_img485</td>\n",
       "      <td>3</td>\n",
       "      <td>POLYGON ((-0.0 153.61 0,-0.0 177.39 0,25.13 18...</td>\n",
       "      <td>POLYGON ((2.242571399947189 49.02272116311444 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AOI_3_Paris_img485</td>\n",
       "      <td>4</td>\n",
       "      <td>POLYGON ((69.9 124.55 0,29.41 110.46 0,2.29 14...</td>\n",
       "      <td>POLYGON ((2.242760130000022 49.022799603000067...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AOI_3_Paris_img485</td>\n",
       "      <td>5</td>\n",
       "      <td>POLYGON ((84.69 190.02 0,157.39 210.2 0,171.31...</td>\n",
       "      <td>POLYGON ((2.242800060000036 49.022622857000044...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223398</th>\n",
       "      <td>AOI_2_Vegas_img4107</td>\n",
       "      <td>4</td>\n",
       "      <td>POLYGON ((650.0 509.9 0,650.0 443.3 0,321.98 5...</td>\n",
       "      <td>POLYGON ((-115.202217599790103 36.211160967734...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223399</th>\n",
       "      <td>AOI_2_Vegas_img4107</td>\n",
       "      <td>5</td>\n",
       "      <td>POLYGON ((650.0 359.73 0,650.0 294.18 0,254.17...</td>\n",
       "      <td>POLYGON ((-115.202217599790103 36.211566416926...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223400</th>\n",
       "      <td>AOI_2_Vegas_img4107</td>\n",
       "      <td>6</td>\n",
       "      <td>POLYGON ((639.48 150.23 0,125.53 392.7 0,136.6...</td>\n",
       "      <td>POLYGON ((-115.202245993999952 36.212132067000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223401</th>\n",
       "      <td>AOI_2_Vegas_img4107</td>\n",
       "      <td>7</td>\n",
       "      <td>POLYGON ((326.22 149.0 0,65.42 270.22 0,77.4 2...</td>\n",
       "      <td>POLYGON ((-115.203091813999947 36.212135405000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223402</th>\n",
       "      <td>AOI_2_Vegas_img2131</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((483.52 648.3 0,483.76 650.0 0,512.79...</td>\n",
       "      <td>POLYGON ((-115.246542082999952 36.175687279000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223403 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ImageId  BuildingId  \\\n",
       "0        AOI_3_Paris_img485           1   \n",
       "1        AOI_3_Paris_img485           2   \n",
       "2        AOI_3_Paris_img485           3   \n",
       "3        AOI_3_Paris_img485           4   \n",
       "4        AOI_3_Paris_img485           5   \n",
       "...                     ...         ...   \n",
       "223398  AOI_2_Vegas_img4107           4   \n",
       "223399  AOI_2_Vegas_img4107           5   \n",
       "223400  AOI_2_Vegas_img4107           6   \n",
       "223401  AOI_2_Vegas_img4107           7   \n",
       "223402  AOI_2_Vegas_img2131           1   \n",
       "\n",
       "                                           PolygonWKT_Pix  \\\n",
       "0       POLYGON ((31.68 11.69 0,31.06 11.39 0,44.13 -0...   \n",
       "1       POLYGON ((-0.0 199.48 0,-0.0 216.43 0,10.24 20...   \n",
       "2       POLYGON ((-0.0 153.61 0,-0.0 177.39 0,25.13 18...   \n",
       "3       POLYGON ((69.9 124.55 0,29.41 110.46 0,2.29 14...   \n",
       "4       POLYGON ((84.69 190.02 0,157.39 210.2 0,171.31...   \n",
       "...                                                   ...   \n",
       "223398  POLYGON ((650.0 509.9 0,650.0 443.3 0,321.98 5...   \n",
       "223399  POLYGON ((650.0 359.73 0,650.0 294.18 0,254.17...   \n",
       "223400  POLYGON ((639.48 150.23 0,125.53 392.7 0,136.6...   \n",
       "223401  POLYGON ((326.22 149.0 0,65.42 270.22 0,77.4 2...   \n",
       "223402  POLYGON ((483.52 648.3 0,483.76 650.0 0,512.79...   \n",
       "\n",
       "                                           PolygonWKT_Geo  \n",
       "0       POLYGON ((2.242656935000071 49.023104327000055...  \n",
       "1       POLYGON ((2.242571399947189 49.022597304268373...  \n",
       "2       POLYGON ((2.242571399947189 49.02272116311444 ...  \n",
       "3       POLYGON ((2.242760130000022 49.022799603000067...  \n",
       "4       POLYGON ((2.242800060000036 49.022622857000044...  \n",
       "...                                                   ...  \n",
       "223398  POLYGON ((-115.202217599790103 36.211160967734...  \n",
       "223399  POLYGON ((-115.202217599790103 36.211566416926...  \n",
       "223400  POLYGON ((-115.202245993999952 36.212132067000...  \n",
       "223401  POLYGON ((-115.203091813999947 36.212135405000...  \n",
       "223402  POLYGON ((-115.246542082999952 36.175687279000...  \n",
       "\n",
       "[223403 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look for information in the datafile\n",
    "csv_res = train_path / 'Building_Solutions.csv'\n",
    "\n",
    "def generate_csv(csv_res):\n",
    "    csv_path = train_path / 'info'\n",
    "    dfs = [pd.read_csv(csv_file) for csv_file in csv_path.iterdir()]\n",
    "    result = pd.concat(dfs)\n",
    "    result.to_csv(csv_res, index=None)\n",
    "    return result\n",
    "\n",
    "# Use function to load csv\n",
    "# df = generate_csv(csv_res)\n",
    "df = pd.read_csv(csv_res)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris : 1148 / 10593\n",
      "Shanghai : 4582 / 10593\n",
      "Khartoum : 1012 / 10593\n",
      "Vegas : 3851 / 10593\n"
     ]
    }
   ],
   "source": [
    "cities = [\"Paris\", \"Shanghai\", \"Khartoum\", \"Vegas\"]\n",
    "\n",
    "for city in cities:\n",
    "    print(city, ':', \n",
    "        df[df['ImageId'].str.contains(city)]['ImageId'].unique().size, \n",
    "        '/', df['ImageId'].unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe38116ff854a60af847dc15141642f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Folder peeling'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='2â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'Paris': 1148, 'Shanghai': 4582, 'Khartoum': 1012, 'Vegas': 3851}\n",
      "Images:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d1bbecac584a158883faee1beef810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Folder peeling'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='2â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MUL-PanSharpen {'Paris': 1148, 'Shanghai': 4582, 'Khartoum': 1012, 'Vegas': 3851}\n"
     ]
    }
   ],
   "source": [
    "def count_files_per_city(fp):\n",
    "    count_dict = {city: 0 for city in cities}\n",
    "\n",
    "    for filename in tqdm(fp.iterdir(), desc=f\"Folder peeling\"):\n",
    "        for city in cities:\n",
    "            if city in filename.stem:\n",
    "                count_dict[city] += 1\n",
    "    return count_dict\n",
    "\n",
    "# GeoJSONs\n",
    "print(\"GeoJSON\")\n",
    "print(count_files_per_city((train_path / \"buildings\")))\n",
    "\n",
    "# Img\n",
    "print(\"Images:\")\n",
    "# print(\"MUL\", count_files_per_city((train_path / \"data\" / \"MUL\" / \"MUL\")))\n",
    "print(\"MUL-PanSharpen\", count_files_per_city((train_path / \"data\" / \"MUL-PanSharpen\" / \"MUL-PanSharpen\")))\n",
    "# print(\"PAN\",count_files_per_city((train_path / \"data\" / \"PAN\" / \"PAN\")))\n",
    "# print(\"RGB-Pan\",count_files_per_city((train_path / \"data\" / \"RGB-PanSharpen\" / \"RGB-PanSharpen\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(filepath, n_ch):\n",
    "    sum_channels = np.zeros(n_ch) #8, 3 or 1\n",
    "    std_channels = np.zeros(n_ch)\n",
    "    total_pixel = 0\n",
    "\n",
    "    for img in tqdm((filepath).iterdir(), desc=\"10593\"):\n",
    "        with rasterio.open(img, 'r') as ds:\n",
    "            try:\n",
    "                arr = ds.read() \n",
    "            except:\n",
    "                print(f\"Uh oh, {img.stem} seems to be corrupted...\")\n",
    "            else:\n",
    "                arr = arr.reshape(arr.shape[0], -1)\n",
    "                sum_channels += arr.sum(axis=-1)\n",
    "                total_pixel += arr[0].size\n",
    "            \n",
    "    mean_channels = sum_channels / total_pixel\n",
    "\n",
    "\n",
    "    for img in tqdm((filepath).iterdir()):\n",
    "        with rasterio.open(img, 'r') as ds:\n",
    "            try:\n",
    "                arr = ds.read() \n",
    "            except:\n",
    "                print(f\"Uh oh, {img.stem} seems to be corrupted...\")\n",
    "            else:\n",
    "                arr = arr.reshape(arr.shape[0], -1)\n",
    "                std_channels += np.sum((arr - mean_channels.reshape(n_ch, 1)) ** 2, axis=-1) \n",
    "            \n",
    "    std_channels = np.sqrt(std_channels / total_pixel)\n",
    "\n",
    "    stats = {'mean': mean_channels.tolist(), 'std': std_channels.tolist()}\n",
    "    return stats\n",
    "\n",
    "# folder_path = train_path / \"data\" / \"MUL-PanSharpen\" / \"MUL-PanSharpen\"\n",
    "# stats = compute_mean_std(folder_path,8)\n",
    "# with open(train_path / 'stats_mul_pan.json', 'w') as file:\n",
    "#     json.dump(stats, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[296.60002065 357.02413693 463.62619325 416.1747543  331.35564105\n",
      " 408.23318352 478.25544466 363.4427353 ]\n",
      "[105.56921283 148.62490128 224.65445921 226.26444245 195.07064322\n",
      " 210.31440212 239.627737   197.79631543]\n"
     ]
    }
   ],
   "source": [
    "def load_stats(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        n_params = json.load(file)\n",
    "    mean_channels = np.array(n_params['mean'])\n",
    "    std_channels = np.array(n_params['std'])\n",
    "    return mean_channels, std_channels\n",
    "\n",
    "mean_channels, std_channels = load_stats(train_path / 'stats_mul_pan.json')\n",
    "print(mean_channels)\n",
    "print(std_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "def norm_img(img, mean_arr, std_arr):\n",
    "    res = (np.transpose(img, (1, 2, 0)) - mean_arr) / std_arr\n",
    "    return np.transpose(res, (2,0,1))\n",
    "\n",
    "def load_tif(fn, df, mean_vec, std_vec):\n",
    "    img_id = \"_\".join(pathlib.Path(fn).stem.split(\"_\")[1:]) # get img id\n",
    "    train_path = pathlib.Path(fn).parents[3] # Get train path from img\n",
    "\n",
    "    no_building = df[df['BuildingId']==-1]['ImageId'].unique().tolist()\n",
    "    geojson_path = train_path / \"buildings\" / f\"buildings_{img_id}.geojson\"\n",
    "\n",
    "    # Extract the file as a (8 x 650 x 650) cube\n",
    "    with rasterio.open(fn) as tif:\n",
    "        arr = tif.read()\n",
    "        info = tif.meta\n",
    "    \n",
    "    info['count'] = 1\n",
    "    # Extract geofeatures if the image has buildings\n",
    "    if img_id in no_building:\n",
    "        X = np.zeros((info['height'],info['width']), dtype='uint16')\n",
    "        features = []\n",
    "    else:\n",
    "        with fiona.open(geojson_path, \"r\") as geojson:\n",
    "            features = [feature[\"geometry\"] for feature in geojson]\n",
    "        X = np.ones((info['height'],info['width']), dtype='uint16')\n",
    "\n",
    "    # Write polygons as a tif whose dimensions are the same than the opened tif\n",
    "    with rasterio.open('temp.tif','w', **info) as new_ds:\n",
    "        new_ds.write(X, 1)\n",
    "    \n",
    "    # Extract mask if necessary\n",
    "    with rasterio.open('temp.tif') as tif:\n",
    "        if features:\n",
    "            mask_img, _ = rasterio.mask.mask(tif, features)\n",
    "        else:\n",
    "            mask_img = tif.read()\n",
    "    \n",
    "    \n",
    "    # arr = norm_img(arr, mean_vec, std_vec)\n",
    "    arr, mask_img = arr.astype('float32'), mask_img.squeeze().astype('int64')\n",
    "    pathlib.Path('temp.tif').unlink()\n",
    "\n",
    "    return arr, mask_img\n",
    "\n",
    "# Need to standardize by avg / std and show as tensor\n",
    "load_img = partial(\n",
    "    load_tif,\n",
    "    df = df, #df directly\n",
    "    mean_vec = mean_channels,\n",
    "    std_vec = std_channels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NÂ° of images: 10593\n",
      "Type of img: MUL-PanSharpen\n"
     ]
    }
   ],
   "source": [
    "img_path = train_path / 'data' / 'MUL-PanSharpen'\n",
    "\n",
    "# Define dataset here\n",
    "ds = datasets.DatasetFolder(root=img_path, \n",
    "                       loader=load_img, \n",
    "                       extensions=('.tif',))\n",
    "\n",
    "print(\"NÂ° of images:\", len(ds))\n",
    "print(\"Type of img:\", ds.classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8474\n",
      "2119\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(ds, train_size=0.8, random_seed=0):\n",
    "    if type(train_size) is float:\n",
    "        train_size = int(len(ds)*train_size)\n",
    "    train_ds, val_ds = random_split(ds, (train_size, len(ds)-train_size), generator=torch.Generator().manual_seed(random_seed))\n",
    "    return train_ds, val_ds\n",
    "\n",
    "train_ds, val_ds = split_dataset(ds, train_size=0.8, random_seed=123)\n",
    "print(len(train_ds))\n",
    "print(len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NÂ° of iterations per batch (train): 530\n",
      "NÂ° of iterations per batch (val): 133\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16 #16\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "print(\"NÂ° of iterations per batch (train):\", len(train_dl))\n",
    "print(\"NÂ° of iterations per batch (val):\", len(val_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the model\n",
    "\n",
    "The model used is an AE (autoencoder) trained specially for the pan-sharpened part of the multichannel dataset, thus for `8x650x650` images. \n",
    "\n",
    "Some modifications might be done in order to exploit this model for other datasets, like changing the number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on whole dataset\n",
    "\n",
    "class AutoencoderBuildingMulPs(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_init = nn.Conv2d(8, 16, 3, padding=1)\n",
    "        self.conv1 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(16, 8, 3, padding=1)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(8, 16, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(16, 16, 3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(32, 16, 3, padding=1)\n",
    "        self.conv_last = nn.Conv2d(16, 2, 3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = F.relu(self.conv_init(x))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x,2)\n",
    "        \n",
    "        # Decoder\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        n_size = tuple([(dim+1)*2 for dim in x.shape[2:]])\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.interpolate(x, mode='bilinear', \n",
    "                          align_corners=False, size=n_size)\n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = self.conv_last(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoencoderBuildingMulPs(\n",
      "  (conv_init): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv7): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv8): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_last): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "ae_model = AutoencoderBuildingMulPs()\n",
    "print(ae_model)\n",
    "\n",
    "#Send to GPU\n",
    "device = torch.device((\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(device)\n",
    "ae_model = ae_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor([.11, .89]))\n",
    "criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(ae_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AutoencoderBuildingMulPs(\n",
      "  (conv_init): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv7): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv8): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_last): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "On: cuda\n",
      "NÂ°epochs: 100\n"
     ]
    }
   ],
   "source": [
    "#Defining a class where we register every parameter necessary to train the model\n",
    "class ModelParameters:\n",
    "    def __init__(self, model, device, epochs, criterion, optimizer, train_dl, val_dl, sim_bs=None):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.criterion = criterion.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        if sim_bs:\n",
    "            self.sim_bs = sim_bs // self.train_dl.batch_size \n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Model: {self.model}\\nOn: {self.device}\\nNÂ°epochs: {epochs}\"\n",
    "\n",
    "mp = ModelParameters(ae_model, device, epochs, criterion, optimizer, \n",
    "                     train_dl, val_dl)\n",
    "print(mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval cell\n",
    "\n",
    "def n_correct_pred(output, label):\n",
    "    bs, ch, h, w = output.shape\n",
    "    pred = output.argmax(dim=1)\n",
    "    return (pred==label).sum() / h*w\n",
    "\n",
    "def batch_cm(output, label):\n",
    "    to_numpy = lambda tens: tens.cpu().detach().reshape(-1).numpy()\n",
    "    cm = np.zeros((2,2))\n",
    "    pred = output.argmax(dim=1)\n",
    "    bs, ch, h, w = output.shape\n",
    "    # use zip instead\n",
    "    for i in range(bs):\n",
    "        cm += confusion_matrix(to_numpy(label[i]), \n",
    "                          to_numpy(pred[i]), labels=[0, 1])\n",
    "    return cm\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(mp):\n",
    "    mp.model.eval()\n",
    "    output_loss, n_corr_preds, total_cm = 0, 0, []\n",
    "    \n",
    "    for batch_n, ((img, mask), _) in tqdm(enumerate(mp.val_dl), desc=\"Model evaluation\", unit=\"batch\", total=len(mp.val_dl)):\n",
    "        img, mask = img.to(mp.device), mask.to(mp.device)\n",
    "        pred = mp.model(img)\n",
    "        \n",
    "        loss = mp.criterion(pred, mask)\n",
    "        output_loss += loss.detach().item()\n",
    "        corr_pred = n_correct_pred(pred, mask)\n",
    "        n_corr_preds += corr_pred.detach().item()\n",
    "        cm = batch_cm(pred,mask)\n",
    "        total_cm.append(cm)\n",
    "    \n",
    "    return output_loss, n_corr_preds / len(mp.val_dl), total_cm\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model_limit(mp, lim):\n",
    "    mp.model.eval()\n",
    "    total_size, output_loss, n_corr_preds = 0, 0, 0\n",
    "    tot_bs = mp.train_dl.batch_size\n",
    "    \n",
    "    for batch_n, ((img, mask), _) in tqdm(enumerate(mp.train_dl), desc=\"Val training\", total=lim, unit=\"batch\"):\n",
    "        img, mask = img.to(mp.device), mask.to(mp.device)\n",
    "        pred = mp.model(img)\n",
    "        total_size += len(pred.view(-1))\n",
    "        loss = mp.criterion(pred, mask)\n",
    "        output_loss += loss.detach().item()\n",
    "\n",
    "        corr_pred = n_correct_pred(pred, mask)\n",
    "        n_corr_preds += corr_pred.detach().item()\n",
    "\n",
    "        if batch_n==lim:\n",
    "            cm = batch_cm(pred,mask)\n",
    "            break\n",
    "    \n",
    "    return output_loss, n_corr_preds / lim, 100* cm / (cm.sum() * tot_bs)\n",
    "\n",
    "\n",
    "def train_model(mp):\n",
    "    (train_path /'save_states').mkdir(exist_ok=True)\n",
    "    \n",
    "    total_results = {'train': {'loss': []}, \n",
    "                   'val': {'loss': [], 'correct_pred': [], 'cm': []}}\n",
    "  \n",
    "\n",
    "    mp.model.zero_grad()\n",
    "    for epoch in trange(mp.epochs, desc=\"Train\", unit=\"epoch\"):\n",
    "        train_epoch = 0\n",
    "        for bn, ((img, mask), _) in tqdm(enumerate(mp.train_dl), \n",
    "                                     desc=f\"Batch training\", \n",
    "                                     total=len(mp.train_dl), unit=\"batch\"):\n",
    "            img, mask = img.to(mp.device), mask.to(mp.device)\n",
    "            mp.model.train()\n",
    "            pred = mp.model(img)\n",
    "            loss = mp.criterion(pred, mask) #Avg loss on whole batch\n",
    "            loss.backward()\n",
    "\n",
    "            train_epoch += loss.detach().item()\n",
    "#             test_batch, pred_batch, cm_batch = eval_model_limit(mp, 5)\n",
    "\n",
    "#             print(f\"Epoch {epoch}, batch {bn}, average loss: {np.array(test_batch).mean()}\")\n",
    "\n",
    "#             iou = cm_batch[-1, -1] / (cm_batch.sum() - cm_batch[0, 0])\n",
    "#             print(\"IoU:\", iou, '%')\n",
    "\n",
    "            mp.model.train()\n",
    "            mp.optimizer.step()                            \n",
    "            mp.optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "#         torch.save({'epoch': epoch,\n",
    "#                'model_state_dict': mp.model.state_dict(),\n",
    "#                'optimizer_state_dict': mp.optimizer.state_dict()}, \n",
    "#                train_path /'save_states'/ f'model_chkpt_ep_{epoch:03d}')\n",
    "\n",
    "    \n",
    "        total_results['train']['loss'].append(train_epoch)\n",
    "        print(f\"Train loss: {train_epoch}\")\n",
    "\n",
    "        # Evaluation step \n",
    "        output_loss, n_corr_preds, total_cm = evaluate_model(mp)\n",
    "        \n",
    "        last_cm = total_cm[-1]\n",
    "        print(f\"Val loss: {output_loss}\")\n",
    "        print(f\"N of correct preds: {n_corr_preds}\")\n",
    "        print(f\"Last CM:\\n{total_cm[-1]}\")\n",
    "        \n",
    "        iou = last_cm[-1, -1] / (last_cm.sum() - last_cm[0, 0])\n",
    "        print(f\"Last IoU: {iou}\")\n",
    "        \n",
    "        total_results['val']['loss'].append(output_loss)\n",
    "        total_results['val']['correct_pred'].append(n_corr_preds)\n",
    "        total_results['val']['cm'].append(total_cm)\n",
    "\n",
    "        torch.save({'epoch': epoch,\n",
    "               'model_state_dict': mp.model.state_dict(),\n",
    "               'optimizer_state_dict': mp.optimizer.state_dict(),\n",
    "               'total_results': total_results}, train_path /'save_states'/ f'model_bs_{mp.train_dl.batch_size}_chkpt_ep_{epoch:03d}')\n",
    "  \n",
    "    torch.save(mp.model.state_dict(), train_path / f'ae_building_mask')\n",
    "    \n",
    "    return total_results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5606b055e04724ae078abcbe3619f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82584bf612343a39a9d6c420dc93869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batch training'), FloatProgress(value=0.0, max=530.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 242.66615882515907\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f409c97bc1f14df2881f9ea038652bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Model evaluation'), FloatProgress(value=0.0, max=133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:81.)\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val loss: 49.972219586372375\n",
      "N of correct preds: 4964734.2105263155\n",
      "Last CM:\n",
      "[[1806218.  666180.]\n",
      " [  17306.  467796.]]\n",
      "Last IoU: 0.40632616509247954\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba93c115c424e56bc9530b62ee62b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batch training'), FloatProgress(value=0.0, max=530.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 205.59057426452637\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f95ae5d90724dae9b986c629f01c2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Model evaluation'), FloatProgress(value=0.0, max=133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val loss: 47.885592103004456\n",
      "N of correct preds: 4781057.894736842\n",
      "Last CM:\n",
      "[[1743354.  581176.]\n",
      " [  45694.  587276.]]\n",
      "Last IoU: 0.48369471216805887\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159c4f7a1a424bed86c610c3043fadb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batch training'), FloatProgress(value=0.0, max=530.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 187.66497646272182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ca3fe9f9144203949410ab99b8a3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Model evaluation'), FloatProgress(value=0.0, max=133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val loss: 45.61543545126915\n",
      "N of correct preds: 5097387.969924812\n",
      "Last CM:\n",
      "[[1673832.  870334.]\n",
      " [  31068.  382266.]]\n",
      "Last IoU: 0.29779195243629974\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685f2c77e23b46eea34737465d75a1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batch training'), FloatProgress(value=0.0, max=530.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "total_results = train_model(mp)\n",
    "\n",
    "\n",
    "with open('total_result_dict.json', 'w') as fp:\n",
    "    json.dump(total_results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_results = total_results.copy()\n",
    "\n",
    "with open('total_result_dict.json', 'w') as fp:\n",
    "    serial_results['val']['cm'] = [[cm.tolist()  for cm in ep] for ep in total_results['val']['cm']]\n",
    "    json.dump(serial_results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(total_results[\"train\"][\"loss\"])\n",
    "plt.plot(total_results[\"val\"][\"loss\"])\n",
    "plt.title(\"Loss evolution\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.savefig(\"loss_value.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_results[\"val\"][\"correct_pred\"])\n",
    "plt.title(\"N of correct predictions\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"N correct prediction\")\n",
    "plt.savefig(\"corr_preds.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cm = total_results['val']['cm']\n",
    "cm_prds = [sum([np.array(cm) for cm in ep]) for ep in total_cm]\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18,12))\n",
    "\n",
    "for i, axrow in enumerate(axs):\n",
    "    for j, ax in enumerate(axrow):\n",
    "        idx = i*3 + j\n",
    "        sns.heatmap(cm_prds[idx*5] / (cm_prds[idx*5].sum()), ax=ax, annot=True)\n",
    "#         sns.heatmap(cm_prds[idx*5] / (cm_prds[idx*5].sum() * mp.val_dl.batch_size*len(mp.val_dl)), ax=ax, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou = [cm[-1, -1] / (cm.sum() - cm[0, 0])  for cm in cm_prds]\n",
    "iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(iou)\n",
    "plt.title(\"IoU\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.savefig(\"iou.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chpt_list = [fn for fn in (train_path /'save_states').iterdir() if fn.name.startswith(\"model_bs_16\")]\n",
    "\n",
    "last_chpt = sorted(chpt_list, key=lambda x: int(x.name.split(\"_\")[-1]))[-1]\n",
    "last_chpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_training(mp, last_chpt):\n",
    "    checkpoint = torch.load(last_chpt)\n",
    "    mp.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    mp.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    total_results = checkpoint[\"total_results\"]\n",
    "    \n",
    "    for epoch in trange(start_epoch + 1, mp.epochs + start_epoch, desc=\"Train\", unit=\"epoch\"):\n",
    "        train_epoch = 0\n",
    "        for bn, ((img, mask), _) in tqdm(enumerate(mp.train_dl), \n",
    "                                     desc=f\"Batch training\", \n",
    "                                     total=len(mp.train_dl), unit=\"batch\"):\n",
    "            img, mask = img.to(mp.device), mask.to(mp.device)\n",
    "            mp.model.train()\n",
    "            pred = mp.model(img)\n",
    "            loss = mp.criterion(pred, mask) #Avg loss on whole batch\n",
    "            loss.backward()\n",
    "\n",
    "            train_epoch += loss.detach().item()\n",
    "            \n",
    "\n",
    "            mp.model.train()\n",
    "            mp.optimizer.step()                            \n",
    "            mp.optimizer.zero_grad()\n",
    "            \n",
    "\n",
    "    \n",
    "        total_results['train']['loss'].append(train_epoch)\n",
    "        print(f\"Train loss: {train_epoch}\")\n",
    "\n",
    "        # Evaluation step \n",
    "        output_loss, n_corr_preds, total_cm = evaluate_model(mp)\n",
    "        \n",
    "        last_cm = total_cm[-1]\n",
    "        print(f\"Val loss: {output_loss}\")\n",
    "        print(f\"N of correct preds: {n_corr_preds}\")\n",
    "        print(f\"Last CM:\\n{total_cm[-1]}\")\n",
    "        \n",
    "        iou = last_cm[-1, -1] / (last_cm.sum() - last_cm[0, 0])\n",
    "        print(f\"Last IoU: {iou}\")\n",
    "        \n",
    "        total_results['val']['loss'].append(output_loss)\n",
    "        total_results['val']['correct_pred'].append(n_corr_preds)\n",
    "        total_results['val']['cm'].append(total_cm)\n",
    "\n",
    "        torch.save({'epoch': epoch,\n",
    "               'model_state_dict': mp.model.state_dict(),\n",
    "               'optimizer_state_dict': mp.optimizer.state_dict(),\n",
    "               'total_results': total_results}, train_path /'save_states'/ f'model_bs_{mp.train_dl.batch_size}_chkpt_ep_{epoch:03d}')\n",
    "  \n",
    "    torch.save(mp.model.state_dict(), train_path / f'ae_building_mask')\n",
    "    \n",
    "    return total_results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "total_results = train_model(mp)\n",
    "\n",
    "\n",
    "with open('total_result_dict.json', 'w') as fp:\n",
    "    json.dump(total_results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
